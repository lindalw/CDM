{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/emmahokken/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising vocab from file.\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import copy\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "from Vocab import Vocab\n",
    "vocab = Vocab('data/vocab.csv', 3)\n",
    "from get_predictions import get_predictions\n",
    "from helpers import *\n",
    "\n",
    "from ChainDataset import ChainDataset\n",
    "\n",
    "chain_test_set = ChainDataset(\n",
    "    data_dir='data/',\n",
    "    segment_file='segments.json',\n",
    "    chain_file='test_chains.json',\n",
    "    vectors_file='vectors.json',\n",
    "    split='test'\n",
    ")\n",
    "\n",
    "from SegmentDataset import SegmentDataset\n",
    "\n",
    "segment_test_set = SegmentDataset(\n",
    "    data_dir='data/',\n",
    "    segment_file='segments.json',\n",
    "    vectors_file='vectors.json',\n",
    "    split='test'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tags distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/emmahokken/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/emmahokken/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /Users/emmahokken/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/emmahokken/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'grey': 'gray',\n",
       " 'ok.': 'OK',\n",
       " 't-shirt': 'tshirt',\n",
       " 'w/': 'with',\n",
       " 'frige': 'fridge',\n",
       " 'red/white': 'red and white',\n",
       " 'doughnuts': 'donuts',\n",
       " 'didnt': \"didn't\",\n",
       " 'ok..': 'ok',\n",
       " 'dr.': 'dr',\n",
       " 'no..': 'no',\n",
       " 'doughnut': 'donut',\n",
       " 'yes..': 'yes',\n",
       " 'yellow/orange': 'yellow and orange',\n",
       " 'pic..': 'picture',\n",
       " 'umberlla': 'umbrella',\n",
       " 'cest': 'chest',\n",
       " 'k..next': 'ok, next',\n",
       " 'surboard': 'surfboard',\n",
       " 'blue/white': 'blue and white',\n",
       " 'man/woman': 'man or woman',\n",
       " 'doesnt': \"doesn't\",\n",
       " 'scheppa': 'schepps',\n",
       " 'guy/girl': 'guy or girl',\n",
       " 'blue/green': 'blue and green',\n",
       " 'schepp': 'schepps',\n",
       " 'blue/black': 'blue and black',\n",
       " 'motarcycle': 'motorcycle',\n",
       " 'red/black': 'red and black',\n",
       " 'red/white/blue': 'red, white and blue',\n",
       " 'colour': 'color',\n",
       " 'close-up': 'closeup',\n",
       " 'green/yellow': 'green and yellow',\n",
       " 'red/pink': 'red and pink',\n",
       " 'buddist': 'buddhist',\n",
       " 'longsleeves': 'long sleeves',\n",
       " 'ihave': 'i have',\n",
       " 'yellow/green': 'yellow and green',\n",
       " 'no-': 'no',\n",
       " 'k.': 'ok',\n",
       " 'surfobard': 'surfboard',\n",
       " 'candleabra': 'candelabra',\n",
       " 'w/bear': 'with bear',\n",
       " 'olw': 'owl',\n",
       " 'white/pink': 'white and pink',\n",
       " 'pink/red': 'pink and red',\n",
       " 'squating': 'squatting',\n",
       " 'teddie': 'teddy',\n",
       " 'white/yellow': 'white and yellow',\n",
       " 'labtop': 'laptop',\n",
       " 'white/red': 'white and red',\n",
       " 'black/white': 'black and white',\n",
       " 'w/red': 'with red',\n",
       " 'yes/no': 'yes or no',\n",
       " 'all-blue': 'all blue',\n",
       " 'hilighted': 'highlighted',\n",
       " 'scoccer': 'soccer',\n",
       " 'shirt*': 'shirt',\n",
       " 'it*': 'it',\n",
       " 'cake..': 'cake',\n",
       " 'isnt': \"isn't\",\n",
       " 'pplaying': 'playing',\n",
       " 'pick-up': 'pickup',\n",
       " 'purple/blue': 'purpble and blue',\n",
       " 'bhudist': 'buddhist',\n",
       " 'van/truck': 'van or truck',\n",
       " 'common/different': 'common or different',\n",
       " 'plad': 'plaid',\n",
       " 'orange/yellow': 'orange and yellow',\n",
       " 'no..to': 'no to',\n",
       " 't-shirts': 'tshirts',\n",
       " 'orange/red': 'orange and red',\n",
       " 'yellow/white': 'yellow and white',\n",
       " 'hightlighted': 'highlighted',\n",
       " 'bike/green': 'green bike',\n",
       " 'train/bus': 'train or bus',\n",
       " 'orrange': 'orange',\n",
       " 'blue/purple': 'blue and purple',\n",
       " 'oh..': 'oh',\n",
       " 'holding/looking': 'holding and looking',\n",
       " 'truck/garbage': 'gargbage truck',\n",
       " 'mercedes-benz': 'mercedes_benz',\n",
       " 'coloured': 'colored',\n",
       " 'backgroun': 'background',\n",
       " 'one/': 'one',\n",
       " '.yes': 'yes',\n",
       " 'silver/black': 'silber and black',\n",
       " 'light-green': 'light green',\n",
       " 'bannanas': 'bananas',\n",
       " 'galss': 'glass',\n",
       " 'pokadot': 'polkadot',\n",
       " '*do': 'do',\n",
       " 'phoen': 'phone',\n",
       " 'palte': 'plaid',\n",
       " 'whuite': 'white',\n",
       " 'frnt': 'front',\n",
       " 'firdge': 'fridge',\n",
       " 'friut': 'fruit',\n",
       " 'silver/white': 'silber and white',\n",
       " 'bus/subway': 'bus and subway',\n",
       " 'long-sleeved': 'long sleeved',\n",
       " 'oven-': 'oven',\n",
       " 'frisbe': 'frisbee',\n",
       " 'nexxt': 'next',\n",
       " 'fedding': 'feeding',\n",
       " 'tan/white': 'tan or white',\n",
       " 'japenese': 'japanese',\n",
       " 'pinapple': 'pineapple',\n",
       " \"'road\": 'road',\n",
       " 'w/green': 'with green',\n",
       " 'whtie': 'white',\n",
       " 'hot-dog': 'hotdog',\n",
       " 'colorfull': 'colorful',\n",
       " 'highlited': 'highlighted',\n",
       " 'truck/bus': 'truck or bus',\n",
       " 'bycicle': 'bicycle',\n",
       " 'subway/train': 'subway or train',\n",
       " 'reask': 'ask again',\n",
       " 'baret': 'barret',\n",
       " 'u.s.': 'US',\n",
       " 'wiskers': 'whiskers',\n",
       " '..and': 'and',\n",
       " 'sirt': 'shirt',\n",
       " 'hat*': 'hat',\n",
       " 'motercycle': 'motorcycle',\n",
       " 'cak': 'cake',\n",
       " 'afgan': 'afghan',\n",
       " 'ok..next': 'ok, next',\n",
       " 'hant': \"hasn't\",\n",
       " 'haveguy': 'have a guy',\n",
       " 'shirt..': 'shirt',\n",
       " 'red/orange': 'red and orange',\n",
       " 'overn': 'oven',\n",
       " 'questions/': 'questions',\n",
       " 'cloth..': 'cloth',\n",
       " 'pot..': 'pot',\n",
       " 'handstanding': 'doing a handstand',\n",
       " 'nvm': 'never mind',\n",
       " 'sungalsses': 'sunglasses',\n",
       " 'bus/train': 'bus or train',\n",
       " 'now*': 'now',\n",
       " '*blue': 'blue',\n",
       " 'allyway': 'alleyway',\n",
       " 'unbrella': 'umbrella',\n",
       " 'filngers': 'fingers',\n",
       " 'bllue': 'blue',\n",
       " 'nmo': 'no',\n",
       " '*no': 'no',\n",
       " 'beer/muffin': 'beer and muffin',\n",
       " 'coool': 'cool',\n",
       " 'truck..': 'truck',\n",
       " 'right..': 'right',\n",
       " '*camera': 'camera',\n",
       " 'candels': 'candel',\n",
       " 'k..': 'ok',\n",
       " 'dad/daughter': 'dad and daughter',\n",
       " 'skirt*': 'skirt',\n",
       " 'cusions': 'cushions',\n",
       " 'furnityre': 'furniture',\n",
       " 'oky': 'okay',\n",
       " 'guy*': 'guy',\n",
       " 'banans': 'bananas',\n",
       " 'time/': 'time',\n",
       " 'manin': 'main',\n",
       " 'it..next': 'it. next',\n",
       " 'eatem': 'eaten',\n",
       " 'no..i': 'ni, i',\n",
       " 'k..thats': 'ok, thanks',\n",
       " 'suitecase': 'suitcase',\n",
       " 'helme': 'helmet',\n",
       " 'salvationarmy': 'salvation army',\n",
       " 'sandwitch': 'sandwich',\n",
       " 'standing/sitting': 'standing or sitting',\n",
       " 'bluie': 'blue',\n",
       " 'ansers': 'answers',\n",
       " 'baby/toddler': 'baby or toddler',\n",
       " 'orry': 'sorry',\n",
       " 'have*': 'have',\n",
       " 'frissbee': 'frisbee',\n",
       " 'child/toddler': 'child or toddler',\n",
       " 'orange/green': 'orange and green',\n",
       " 'gold/bronze': 'gold or bronze'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "import pickle\n",
    "from nltk import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open('oov_dictionary.pickle', 'rb') as f:\n",
    "    oov_dict = pickle.load(f)\n",
    "\n",
    "top_10 = ['NOUN', 'DET', 'VERB', 'ADP', 'ADJ', '.', 'ADV', 'NUM', 'PRON', 'CONJ']\n",
    "top_4 = ['NOUN', 'VERB', 'ADJ', 'ADV']\n",
    "\n",
    "''' explanation:\n",
    "\n",
    "ADJ\tadjective\tnew, good, high, special, big, local\n",
    "ADP\tadposition\ton, of, at, with, by, into, under\n",
    "ADV\tadverb\treally, already, still, early, now\n",
    "CONJ\tconjunction\tand, or, but, if, while, although\n",
    "DET\tdeterminer, article\tthe, a, some, most, every, no, which\n",
    "NOUN\tnoun\tyear, home, costs, time, Africa\n",
    "NUM\tnumeral\ttwenty-four, fourth, 1991, 14:24\n",
    "PRT\tparticle\tat, on, out, over per, that, up, with\n",
    "PRON\tpronoun\the, their, her, its, my, I, us\n",
    "VERB\tverb\tis, say, told, given, playing, would\n",
    ".\tpunctuation marks\t. , ; !\n",
    "X\tother\tersatz, esprit, dunno, gr8, univeristy\n",
    "\n",
    "'''\n",
    "\n",
    "oov_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model: No history\n",
      "Initialising vocab from file.\n",
      "vocab len 3424\n",
      "Initialising vocab from file.\n",
      "Loaded seg2ranks and idlist\n",
      "params. normalize=True, mask=True, weight=5.5, weighting=True, batchsize=1, breaking=False\n",
      "Dataparams. data_dir=./data, segmentfile=segments.json, vectorfile=vectors.json, chains_file=chains.json\n",
      "segment dataset with segments.json vectors.json test\n",
      "history dataset with test_segments.json vectors.json test_chains.json test\n",
      "processing test\n",
      "\n",
      "Start prediction\n",
      "predict no history\n",
      "{'segment': [5, 212, 4, 166, 5, 12, 22, 782, 29, 234, 213, 10, 4, 416, 14, 12, 22, 7, 6, 49, 11, 6, 155, 295, 11, 6, 811, 16, 9, 95, 10, 5, 17, 8, 12], 'image_set': ['93469', '380128', '14238', '259745', '524866', '341060', '483794', '96757'], 'targets': [5], 'length': 35, 'preds': tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]]), 'loss': tensor(4.9204), 'ranks': [0]}\n",
      "getting predicitons took 36.962406158447266 seconds\n",
      "loaded model: History\n",
      "Initialising vocab from file.\n",
      "vocab len 3424\n",
      "Initialising vocab from file.\n",
      "Loaded seg2ranks and idlist\n",
      "params. normalize=True, mask=True, weight=5.5, weighting=True, batchsize=1, breaking=False\n",
      "Dataparams. data_dir=./data, segmentfile=segments.json, vectorfile=vectors.json, chains_file=chains.json\n",
      "segment dataset with segments.json vectors.json test\n",
      "history dataset with test_segments.json vectors.json test_chains.json test\n",
      "processing test\n",
      "\n",
      "Start prediction\n",
      "predicting datapoint 0 out of 6801\n",
      "predicting datapoint 500 out of 6801\n",
      "predicting datapoint 1000 out of 6801\n",
      "predicting datapoint 1500 out of 6801\n",
      "predicting datapoint 2000 out of 6801\n",
      "predicting datapoint 2500 out of 6801\n",
      "predicting datapoint 3000 out of 6801\n",
      "predicting datapoint 3500 out of 6801\n",
      "predicting datapoint 4000 out of 6801\n",
      "predicting datapoint 4500 out of 6801\n",
      "predicting datapoint 5000 out of 6801\n",
      "predicting datapoint 5500 out of 6801\n",
      "predicting datapoint 6000 out of 6801\n",
      "predicting datapoint 6500 out of 6801\n",
      "getting predicitons took 314.406724691391 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6801/6801 [00:00<00:00, 40527.18it/s]\n",
      "  0%|          | 0/6801 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 12, 22, 7, 21, 11, 294, 16, 9, 945, 10, 5, 13, 8, 12, 31] [4, 12, 22, 7, 21, 11, 294, 16, 9, 945, 10, 5, 13, 8, 12, 31]\n",
      "No history\n",
      "History\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6801/6801 [00:00<00:00, 44055.38it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_pred_no_hist, dataset_pred_hist_cp, conditions_inds, condition_seg_hist, dataframe = get_pred_datasets()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SegmentDataset.SegmentDataset at 0x1a2e1fb828>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset_pred_no_hist, dataset_pred_hist_cp, conditions_inds, condition_seg_hist\n",
    "\n",
    "dataset_pred_no_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pos_tag_distribution_per_group(group):\n",
    "    segments = list(condition_seg_hist[group])\n",
    "    for segment in tqdm(segments):\n",
    "        if condition_seg_hist[group][segment] == {}:\n",
    "                continue\n",
    "\n",
    "        # Get sentences per segment\n",
    "        first_seg, current_seg = convert_to_sentences(segment, condition_seg_hist[group])\n",
    "        first_sentence_dict = sentence_to_pos_tags(first_seg)\n",
    "        current_sentence_dict = sentence_to_pos_tags(current_seg)\n",
    "\n",
    "        # Update dicts\n",
    "        update_dict(first_sentence_dict, total_first_seg_dict)\n",
    "        update_dict(current_sentence_dict, total_current_dict)\n",
    "\n",
    "    return total_first_seg_dict, total_current_dict\n",
    "\n",
    "def convert_to_sentences(segment, dataset):\n",
    "          \n",
    "    # Decode first and current sentence \n",
    "    first_seg = list(dataset[segment].values())[0]['first_seg']\n",
    "\n",
    "    current_seg = list(dataset[segment].values())[0]['current_seg']\n",
    "    dec_first_seg = vocab.decode(first_seg)\n",
    "    dec_current_seg = vocab.decode(current_seg)\n",
    "    \n",
    "    return dec_first_seg, dec_current_seg\n",
    "\n",
    "def update_dict(new_dict, total_dict):\n",
    "    for key, value in new_dict.items():\n",
    "        new_key = key\n",
    "        new_value = value\n",
    "#         print(\"new key\", new_key)\n",
    "#         print(\"new_value\", new_value)\n",
    "        for key, value in total_dict.items():\n",
    "            if key == new_key:\n",
    "                current_value = total_dict[key] \n",
    "                update_value = current_value + new_value[0]\n",
    "                total_dict[key] = update_value\n",
    "                \n",
    "    return None\n",
    "\n",
    "def dict_to_result(first_seg_dict, group, result, place): #   convert dict to list\n",
    "    \n",
    "    dict_list = []\n",
    "    total = 0\n",
    "    for key, value in first_seg_dict.items():\n",
    "        temp = [value,key]\n",
    "        total += value\n",
    "        dict_list.append(temp)\n",
    "    \n",
    "    # Convert to percentages\n",
    "    for tag in dict_list:\n",
    "        tag[0] = tag[0] / total\n",
    "    \n",
    "    dict_list.sort(reverse = True)\n",
    "    group = group + \"_\" +  place\n",
    "    \n",
    "    result[group] = dict_list\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def sentence_to_pos_tags(sentence):\n",
    "    \n",
    "    lower_case_sentence = []\n",
    "    correct_sentence = [] \n",
    "    pos_tags = []\n",
    "    \n",
    "\n",
    "    for word in sentence:\n",
    "        lower_case_sentence.append(word.lower())\n",
    "    \n",
    "\n",
    "    for word in lower_case_sentence:\n",
    "\n",
    "        if word not in oov_dict:\n",
    "            correct_sentence.append(word)\n",
    "        else:\n",
    "            correct_sentence.append(oov_dict[word])\n",
    "\n",
    "    # Tag the sentence\n",
    "    try:\n",
    "        pos_tags += [word[1] for word in pos_tag(correct_sentence, tagset='universal')]\n",
    "\n",
    "    except:\n",
    "        None\n",
    "\n",
    "        \n",
    "    pos_tag_dict = dict()       \n",
    "    for key,value in dict(Counter(pos_tags)).items():\n",
    "        pos_tag_dict[key] = [value]  \n",
    "\n",
    "    return pos_tag_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 112/112 [00:00<00:00, 207.68it/s]\n",
      "100%|██████████| 248/248 [00:00<00:00, 321.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key hT_nhF_first\n",
      "value [[0.32154088050314467, 'NOUN'], [0.15683962264150944, 'VERB'], [0.13561320754716982, 'DET'], [0.10573899371069183, 'ADP'], [0.09905660377358491, 'ADJ'], [0.07389937106918239, '.'], [0.03970125786163522, 'PRON'], [0.029874213836477988, 'ADV'], [0.02397798742138365, 'CONJ'], [0.013757861635220126, 'NUM']]\n",
      "----\n",
      "key hT_nhF_current\n",
      "value [[0.33168724279835393, 'NOUN'], [0.14814814814814814, 'VERB'], [0.14650205761316873, 'ADJ'], [0.09547325102880659, 'ADP'], [0.09382716049382717, 'DET'], [0.07901234567901234, '.'], [0.03786008230452675, 'PRON'], [0.02633744855967078, 'ADV'], [0.02551440329218107, 'NUM'], [0.015637860082304528, 'CONJ']]\n",
      "----\n",
      "{'NOUN': 0.32154088050314467, 'VERB': 0.15683962264150944, 'DET': 0.13561320754716982, 'ADP': 0.10573899371069183, 'ADJ': 0.09905660377358491, '.': 0.07389937106918239, 'PRON': 0.03970125786163522, 'ADV': 0.029874213836477988, 'CONJ': 0.02397798742138365, 'NUM': 0.013757861635220126}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# groups = ['hT_nhF', 'hT_nhT', 'hF_nhF','hF_nhT']\n",
    "groups = ['hT_nhF', 'hF_nhF']\n",
    "result = {}\n",
    "for group in groups:\n",
    "    \n",
    "    total_first_seg_dict = {\n",
    "        'NOUN': 0,\n",
    "        'DET': 0,\n",
    "        'VERB': 0,\n",
    "        'ADP': 0,\n",
    "        'ADJ': 0,\n",
    "        '.': 0,\n",
    "        'ADV': 0,\n",
    "        'NUM': 0,\n",
    "        'PRON': 0,\n",
    "        'CONJ': 0\n",
    "    }\n",
    "\n",
    "    total_current_dict = {\n",
    "        'NOUN': 0,\n",
    "        'DET': 0,\n",
    "        'VERB': 0,\n",
    "        'ADP': 0,\n",
    "        'ADJ': 0,\n",
    "        '.': 0,\n",
    "        'ADV': 0,\n",
    "        'NUM': 0,\n",
    "        'PRON': 0,\n",
    "        'CONJ': 0  \n",
    "    }\n",
    "    \n",
    "    total_first_seg_dict, total_current_dict = pos_tag_distribution_per_group(group)\n",
    "    result = dict_to_result(total_first_seg_dict, group, result, \"first\")\n",
    "    result = dict_to_result(total_current_dict, group, result, \"current\")\n",
    "\n",
    "dif_hT_nhF = {}\n",
    "dif_hT_nhT = {}\n",
    "dif_hF_nhF = {}\n",
    "dif_hF_nhT = {}\n",
    "\n",
    "# print(result)\n",
    "for key, value in result.items():    \n",
    "    if key.startswith('hT_nhF'):\n",
    "        print(\"key\", key)\n",
    "        print(\"value\", value)\n",
    "        print(\"----\")\n",
    "        \n",
    "        # als de dict leeg is vul dan\n",
    "        if bool(dif_hT_nhF) == False:\n",
    "            for v in value:\n",
    "                dif_hT_nhF[v[1]] = v[0]\n",
    "            \n",
    "            \n",
    "        # als dict al gevuld is .. trek dan de bende er vanaf\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    if key.startswith(\"hT_nhT\"):\n",
    "        None\n",
    "    if key.startswith(\"hF_nhF\"):\n",
    "        None\n",
    "    if key.startswith(\"hF_nhT\"):\n",
    "        None\n",
    "print(dif_hT_nhF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dict(plot_title, values):\n",
    "    labels = []\n",
    "    all_values = []\n",
    "    \n",
    "    for v in range(len(values)):\n",
    "        # print(\"v\", values[v])\n",
    "\n",
    "        labels.append(values[v][1])\n",
    "        all_values.append(values[v][0])\n",
    "\n",
    "    fig1, ax1 = plt.subplots()\n",
    "    ax1.pie(all_values, labels=labels, autopct='%1.1f%%',\n",
    "            shadow=True, startangle=90)\n",
    "    ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "    fig1.suptitle(plot_title, fontsize=20)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new key NOUN\n",
      "new_value 5\n",
      "found NOUN\n",
      "new key ADV\n",
      "new_value 20\n",
      "found ADV\n",
      "{'NOUN': 5, 'DET': 0, 'VERB': 0, 'ADP': 0, 'ADJ': 0, '.': 0, 'ADV': 20, 'NUM': 0, 'PRON': 0, 'CONJ': 0}\n"
     ]
    }
   ],
   "source": [
    "test_dict = {\n",
    "    'NOUN': 0,\n",
    "    'DET': 0,\n",
    "    'VERB': 0,\n",
    "    'ADP': 0,\n",
    "    'ADJ': 0,\n",
    "    '.': 0,\n",
    "    'ADV': 0,\n",
    "    'NUM': 0,\n",
    "    'PRON': 0,\n",
    "    'CONJ': 0\n",
    "}\n",
    "\n",
    "new_dict = {\n",
    "    'NOUN': 5,\n",
    "    'ADV': 20,\n",
    "}\n",
    "\n",
    "for key, value in new_dict.items():\n",
    "    new_key = key\n",
    "    new_value = value\n",
    "    print(\"new key\", new_key)\n",
    "    print(\"new_value\", new_value)\n",
    "    for key, value in test_dict.items():\n",
    "        if key == new_key:\n",
    "            print(\"found\", key)\n",
    "            test_dict[key] += new_value\n",
    "            \n",
    "print(test_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments with leaving out tags in segments :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-e5a11eb71476>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-e5a11eb71476>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    print(f\"Target image index(es): {sample[\"targets\"]}\")\u001b[0m\n\u001b[0m                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing the SegmentDataset class item getter...\")\n",
    "print(f\"Dataset contains {len(segment_test_set)} segment samples\")\n",
    "sample_id = 2\n",
    "sample = segment_test_set[sample_id]\n",
    "print(f\"Segment {sample_id}:\")\n",
    "print(f\"Image set: {sample['image_set']}\")\n",
    "print(f\"Target image index(es): {sample[\"targets\"]}\")\n",
    "# print(\"Target image Features: {}\".format([segment_test_set.image_features[sample[\"image_set\"][int(target)]] for target in sample[\"targets\"]]))\n",
    "print(f\"Encoded segment: {sample['segment']}\")\n",
    "print(f\"Decoded segment dialogue: {vocab.decode(sample[\"segment\"])}\")\n",
    "print(f\"Segment length: {sample[\"length\"]}\")\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the ChainDataset class initialization...\n",
      "Dataset contains 2811 cains.\n",
      "Chain 5:\n",
      "Source Game ID: 3R2UR8A0IBELL07JV1VG7ISBNQAOX23WMINLGALC18GY2NUMUBSANWXRHCAO\n",
      "Target image index: 522129\n",
      "Chain length: 4\n",
      "Segment IDs: [3, 10, 12, 15]\n",
      "Segment lengths:  [23, 23, 21, 19]\n",
      "First segment encoding: [5, 38, 191, 23, 127, 11, 6, 120, 23, 127, 15, 9, 314, 10, 4, 13, 5, 80, 14, 128, 68, 22, 10]\n",
      "First segment decoded dialogue: ['-B-', 'two', 'plates', 'of', 'food', 'with', 'a', 'bowl', 'of', 'food', 'in', 'the', 'middle', '?', '-A-', 'no', '-B-', 'okay', '.', 'how', 'about', 'you', '?']\n",
      "Second segment decoded dialogue: ['-A-', 'do', 'you', 'have', 'the', 'one', 'with', 'two', 'plates', 'and', 'the', 'salsa', 'in', 'the', 'middle', '?', '-B-', 'i', 'do', \"n't\", 'have', 'that', 'one']\n",
      "Reference chain and segments' associated image sets:\n",
      "['-B-', 'two', 'plates', 'of', 'food', 'with', 'a', 'bowl', 'of', 'food', 'in', 'the', 'middle', '?', '-A-', 'no', '-B-', 'okay', '.', 'how', 'about', 'you', '?']\n",
      "['468357', '522129', '93469', '380128', '14238', '259745', '341060', '483794', '96757', '82894']\n",
      "['-A-', 'do', 'you', 'have', 'the', 'one', 'with', 'two', 'plates', 'and', 'the', 'salsa', 'in', 'the', 'middle', '?', '-B-', 'i', 'do', \"n't\", 'have', 'that', 'one']\n",
      "['468357', '522129', '420523', '380128', '14238', '259745', '341060', '96757', '82894']\n",
      "['-A-', 'do', 'you', 'have', 'two', 'plates', 'of', 'food', 'with', 'salsa', 'in', 'the', 'middle', '?', '-B-', 'i', 'do', \"n't\", 'have', 'that', 'one']\n",
      "['468357', '522129', '93469', '420523', '380128', '259745', '524866', '483794']\n",
      "['-B-', 'you', 'go', 'can', 'fo', 'first', '-A-', 'two', 'plates', 'with', 'sauce', 'in', 'the', 'middle', '?', '-B-', 'can', '<unk>', 'yes']\n",
      "['468357', '522129', '93469', '420523', '14238', '524866', '96757', '82894']\n",
      "\n",
      "Done.\n",
      "Testing the ChainDataset class item getter...\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing the ChainDataset class initialization...\")\n",
    "print(f\"Dataset contains {len(chain_test_set.chains)} cains.\")\n",
    "\n",
    "sample_id = 5\n",
    "sample = chain_test_set.chains[sample_id]\n",
    "\n",
    "print(\"Chain {}:\".format(sample_id))\n",
    "print(\"Source Game ID: {}\".format(sample[\"game_id\"]))\n",
    "print(\"Target image index: {}\".format(sample[\"target\"]))\n",
    "print(\"Chain length: {}\".format(len(sample[\"segments\"])))\n",
    "print(\"Segment IDs: {}\".format(sample[\"segments\"]))\n",
    "print(\"Segment lengths: \", sample[\"lengths\"])\n",
    "print(\"First segment encoding: {}\".format(chain_test_set.segments[sample[\"segments\"][0]][\"segment\"]))\n",
    "print(\"First segment decoded dialogue: {}\".format(vocab.decode(chain_test_set.segments[sample[\"segments\"][0]][\"segment\"])))\n",
    "print(\"Second segment decoded dialogue: {}\".format(vocab.decode(chain_test_set.segments[sample[\"segments\"][1]][\"segment\"])))\n",
    "# print(\"third segment decoded dialogue: {}\".format(vocab.decode(chain_test_set.segments[sample[\"segments\"][2]][\"segment\"])))\n",
    "\n",
    "print(\"Reference chain and segments' associated image sets:\")\n",
    "for segment in sample[\"segments\"]:\n",
    "    print(vocab.decode(chain_test_set.segments[segment][\"segment\"]))\n",
    "    print(chain_test_set.segments[segment][\"image_set\"])\n",
    "print(\"\\nDone.\")\n",
    "\n",
    "print(\"Testing the ChainDataset class item getter...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_history_dict(chain_data):\n",
    "    history_dict = {}\n",
    "\n",
    "    # Loop through all segments\n",
    "    for segment in tqdm(range(0, 6801)):\n",
    "        \n",
    "        # For each segment, find its place in the chain_data and add this to history dict\n",
    "        for index, chain_data_row in enumerate(chain_data):\n",
    "            chain_segments = chain_data_row['segments']\n",
    "            \n",
    "            if segment in chain_segments:\n",
    "\n",
    "                # If segment already exits, add new index\n",
    "                if segment in history_dict:\n",
    "                    current_list = history_dict[segment]\n",
    "                    current_list.append(index)\n",
    "                    history_dict[segment] = current_list\n",
    "                # If segment does not yet exist in dict, add it \n",
    "                else:\n",
    "                    history_dict[segment] = [index]\n",
    "                    \n",
    "    return history_dict\n",
    "    # history_dict\n",
    "\n",
    "#     for key, value in history_dict.items():\n",
    "#         print(key)\n",
    "#         print(value)\n",
    "\n",
    "\n",
    "    # geef de segment_id en het returnt de index waar het in staat\n",
    "\n",
    "    # segment_id: [ index]\n",
    "    # {0: [0],\n",
    "    #  7: [0],\n",
    "    #  8: [0, 4],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def update_history_dataset(segment_dataset, history_dict, chain_data):\n",
    "    \n",
    "    for segment_id, segment_data in enumerate(segment_dataset):\n",
    "        segment_length = segment_data['length']\n",
    "        \n",
    "        # Find at which index in history file a certain segment is\n",
    "        index = history_dict[segment_id]\n",
    "    \n",
    "        # print(\"seg id \", segment_id)\n",
    "        # print(\"seg length\", segment_length)\n",
    "    \n",
    "        # print(\"index\", index)\n",
    "    \n",
    "        # Retrieve all indices in history file and change length\n",
    "        for i in index:\n",
    "            # print(\"CHAIN\", chain_data[i])\n",
    "            all_segments_for_that_index = chain_data[i]['segments']\n",
    "            for ii, seg in enumerate(all_segments_for_that_index):\n",
    "                # print(\"ii\", ii)\n",
    "                # print(\"seg\", seg)\n",
    "                if segment_id == seg:\n",
    "                    # print(\"HIER MOET JE LENGTE VERVANGEN\", chain_data[i]['lengths'][0])\n",
    "                    chain_data[i]['lengths'][ii] = segment_length\n",
    "                    # print(\"DAARNA\", chain_data[i])\n",
    "                    \n",
    "    return chain_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_dialogue_data():\n",
    "    segment_test_set = copy.deepcopy(segment_test_set)   \n",
    "    chain_data = copy.deepcopy(chain_test_set.chains)\n",
    "    \n",
    "    return segment_test_set, chain_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tag(tag, segment_test_set, remove_perc=1.0):\n",
    "    for segment_id, segment_data in enumerate(segment_test_set):\n",
    "        \n",
    "        pos_tags = []\n",
    "        lower_case_sentence = []\n",
    "        correct_sentence = []\n",
    "\n",
    "        encoded_segment = vocab.decode(segment_data['segment'])\n",
    "\n",
    "        for word in encoded_segment:\n",
    "            if word == '-A-':\n",
    "                lower_case_sentence.append(word)\n",
    "                continue\n",
    "            if word == '-B-':\n",
    "                lower_case_sentence.append(word)\n",
    "                continue\n",
    "            lower_case_sentence.append(word.lower())\n",
    "\n",
    "        for word in lower_case_sentence:\n",
    "\n",
    "            if word not in oov_dict:\n",
    "                correct_sentence.append(word)\n",
    "            else:\n",
    "                correct_sentence.append(oov_dict[word])\n",
    "\n",
    "        try: \n",
    "            pos_tags += [word[1] for word in pos_tag(correct_sentence, tagset='universal')]\n",
    "\n",
    "            # List with indices to remove\n",
    "            remove_index = []\n",
    "            for index, word in enumerate(pos_tags):\n",
    "                if word == tag:\n",
    "                    remove_index.append(index)\n",
    "\n",
    "            # Only keep certain percentage of indices to remove \n",
    "            remove_index = np.random.choice(remove_index, int(remove_perc*len(remove_index)))\n",
    "          \n",
    "            for index in reversed(remove_index):\n",
    "                del correct_sentence[index]\n",
    "\n",
    "            encoded_sentence = vocab.encode(correct_sentence)\n",
    "\n",
    "            segment_test_set[segment_id]['segment'] = encoded_sentence\n",
    "            segment_test_set[segment_id]['length'] = len(encoded_sentence)\n",
    "\n",
    "        except IndexError:\n",
    "            continue \n",
    "\n",
    "    \n",
    "    return segment_test_set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6801/6801 [00:03<00:00, 2136.85it/s]\n"
     ]
    }
   ],
   "source": [
    "history_dict = create_history_dict(chain_test_set.chains)\n",
    "\n",
    "# Remove tags from nouns\n",
    "segment_test_set_cp, chain_test_set_cp = import_dialogue_data()\n",
    "segment_test_set_noun = remove_tag(\"NOUN\", segment_test_set_cp, 0.5)\n",
    "chain_data_noun = update_history_dataset(segment_test_set_noun, history_dict, chain_test_set_cp)\n",
    "\n",
    "# Remove tags from verbs\n",
    "segment_test_set, chain_data = import_dialogue_data()\n",
    "segment_test_set_verb = remove_tag(\"VERB\", segment_test_set)\n",
    "chain_data_verb = update_history_dataset(segment_test_set_verb, history_dict, chain_data)\n",
    "\n",
    "# Remove tags from adjectives\n",
    "segment_test_set, chain_data = import_dialogue_data()\n",
    "segment_test_set_adj = remove_tag(\"ADJ\", segment_test_set)\n",
    "chain_data_adj = update_history_dataset(segment_test_set_adj, history_dict, chain_data)\n",
    "\n",
    "# Remove tags from adverbs\n",
    "segment_test_set, chain_data = import_dialogue_data()\n",
    "segment_test_set_adv = remove_tag(\"ADV\", segment_test_set)\n",
    "chain_data_adv = update_history_dataset(segment_test_set_adv, history_dict, chain_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write chain information to file\n",
    "with open('data/test_noun_chains.json', 'w') as json_file:\n",
    "    json.dump(chain_data_noun, json_file)   \n",
    "\n",
    "with open('data/test_verb_chains.json', 'w') as json_file:\n",
    "    json.dump(chain_data_verb, json_file)\n",
    "    \n",
    "with open('data/test_adj_chains.json', 'w') as json_file:\n",
    "    json.dump(chain_data_adj, json_file)\n",
    "\n",
    "with open('data/test_adv_chains.json', 'w') as json_file:\n",
    "    json.dump(chain_data_adv, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write segments to json file\n",
    "segment_data_noun_test = [seg for seg in segment_test_set_noun]\n",
    "segment_data_verb_test = [seg for seg in segment_test_set_verb]\n",
    "segment_data_adj_test = [seg for seg in segment_test_set_adj]\n",
    "segment_data_adv_test = [seg for seg in segment_test_set_adv]\n",
    "\n",
    "with open('data/test_noun_segments.json', 'w') as json_file:\n",
    "    json.dump(segment_data_noun_test, json_file)\n",
    "       \n",
    "with open('data/test_verb_segments.json', 'w') as json_file:\n",
    "    json.dump(segment_data_verb_test, json_file)\n",
    "        \n",
    "with open('data/test_adj_segments.json', 'w') as json_file:\n",
    "    json.dump(segment_data_adj_test, json_file)\n",
    "      \n",
    "with open('data/test_adv_segments.json', 'w') as json_file:\n",
    "    json.dump(segment_data_adv_test, json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
