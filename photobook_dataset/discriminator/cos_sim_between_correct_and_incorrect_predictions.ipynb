{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/emmahokken/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising vocab from file.\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import copy\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import scipy.stats as stats\n",
    "\n",
    "from Vocab import Vocab\n",
    "vocab = Vocab('data/vocab.csv', 3)\n",
    "from get_predictions import get_predictions\n",
    "from helpers import *\n",
    "\n",
    "from ChainDataset import ChainDataset\n",
    "\n",
    "chain_test_set = ChainDataset(\n",
    "    data_dir='data/',\n",
    "    segment_file='segments.json',\n",
    "    chain_file='test_chains.json',\n",
    "    vectors_file='vectors.json',\n",
    "    split='test'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_google_300d = api.load('word2vec-google-news-300')\n",
    "wv_twitter_50d = api.load('glove-twitter-50')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model: No history\n",
      "Initialising vocab from file.\n",
      "vocab len 3424\n",
      "Initialising vocab from file.\n",
      "Loaded seg2ranks and idlist\n",
      "params. normalize=True, mask=True, weight=5.5, weighting=True, batchsize=1, breaking=False\n",
      "Dataparams. data_dir=./data, segmentfile=segments.json, vectorfile=vectors.json, chains_file=chains.json\n",
      "processing test\n",
      "\n",
      "Start prediction\n",
      "predict no history\n",
      "{'segment': [5, 212, 4, 166, 5, 12, 22, 782, 29, 234, 213, 10, 4, 416, 14, 12, 22, 7, 6, 49, 11, 6, 155, 295, 11, 6, 811, 16, 9, 95, 10, 5, 17, 8, 12], 'image_set': ['93469', '380128', '14238', '259745', '524866', '341060', '483794', '96757'], 'targets': [5], 'length': 35, 'preds': tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]]), 'loss': tensor(4.9204), 'ranks': [0]}\n",
      "getting predicitons took 56.80236005783081 seconds\n",
      "loaded model: History\n",
      "Initialising vocab from file.\n",
      "vocab len 3424\n",
      "Initialising vocab from file.\n",
      "Loaded seg2ranks and idlist\n",
      "params. normalize=True, mask=True, weight=5.5, weighting=True, batchsize=1, breaking=False\n",
      "Dataparams. data_dir=./data, segmentfile=segments.json, vectorfile=vectors.json, chains_file=chains.json\n",
      "processing test\n",
      "\n",
      "Start prediction\n",
      "predicting datapoint 0 out of 6801\n",
      "predicting datapoint 500 out of 6801\n",
      "predicting datapoint 1000 out of 6801\n",
      "predicting datapoint 1500 out of 6801\n",
      "predicting datapoint 2000 out of 6801\n",
      "predicting datapoint 2500 out of 6801\n",
      "predicting datapoint 3000 out of 6801\n",
      "predicting datapoint 3500 out of 6801\n",
      "predicting datapoint 4000 out of 6801\n",
      "predicting datapoint 4500 out of 6801\n",
      "predicting datapoint 5000 out of 6801\n",
      "predicting datapoint 5500 out of 6801\n",
      "predicting datapoint 6000 out of 6801\n",
      "predicting datapoint 6500 out of 6801\n",
      "getting predicitons took 352.30988907814026 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 6591/6801 [00:00<00:00, 32678.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 12, 22, 7, 21, 11, 294, 16, 9, 945, 10, 5, 13, 8, 12, 31] [4, 12, 22, 7, 21, 11, 294, 16, 9, 945, 10, 5, 13, 8, 12, 31]\n",
      "No history\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6801/6801 [00:00<00:00, 32572.01it/s]\n",
      " 39%|███▉      | 2648/6801 [00:00<00:00, 26464.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6801/6801 [00:00<00:00, 27495.80it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_pred_no_hist, dataset_pred_hist_cp, conditions_inds, condition_seg_hist = get_pred_datasets_orig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = stopwords.words('english')\n",
    "\n",
    "stopword_list.append(\"-B-\")\n",
    "stopword_list.append(\"-A-\")\n",
    "stopword_list.append(\"<unk>\")\n",
    "\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def embed(seg):\n",
    "    ''' Create embedding vector '''\n",
    "    embedding = np.zeros((300,))\n",
    "    length = 0\n",
    "    for word in seg:\n",
    "\n",
    "        # only use word if it is not a stopword \n",
    "        if word in stopword_list:\n",
    "            continue\n",
    "            \n",
    "        # try to embed word and add it to sentence vector \n",
    "        try:\n",
    "            em = wv_google_300d[word]\n",
    "            embedding += em\n",
    "            length += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "    \n",
    "    return embedding / length \n",
    "\n",
    "def get_something(condition, condition_seg_hist):\n",
    "    ''' Calculate mean cosine similarity for all conditions '''\n",
    "    cosine_sim = []\n",
    "    csh_keys = list(condition_seg_hist[condition])\n",
    "    count = 0 \n",
    "    for k in csh_keys:\n",
    "        \n",
    "        # ensure current chain is not empty (TODO: fix this maybe? so this doesn't happen)\n",
    "        if condition_seg_hist[condition][k] == {}:\n",
    "            continue\n",
    "            \n",
    "        # decode first and current sentence \n",
    "        first_seg = list(condition_seg_hist[condition][k].values())[0]['first_seg']\n",
    "        current_seg = list(condition_seg_hist[condition][k].values())[0]['current_seg']\n",
    "        dec_first_seg = vocab.decode(first_seg)\n",
    "        dec_current_seg = vocab.decode(current_seg)\n",
    "    \n",
    "        # embed words  \n",
    "        embed_first = embed(dec_first_seg).reshape(1, -1)\n",
    "        embed_current = embed(dec_current_seg).reshape(1, -1)\n",
    "        \n",
    "        # calculate cosine similarity \n",
    "        try: \n",
    "            cosim = cosine_similarity(embed_first, embed_current)\n",
    "            cosine_sim.append(cosim)\n",
    "        except ValueError:\n",
    "            count += 1\n",
    "            \n",
    "    # show how many times no cosine similarity was able to be calculated\n",
    "    print(count)\n",
    "    \n",
    "    # calculate mean and std \n",
    "    cosine_sim = np.array(cosine_sim)\n",
    "    cosine_sim_mean = cosine_sim.mean()\n",
    "    cosine_sim_std = cosine_sim.std()\n",
    "    \n",
    "    return cosine_sim_mean, cosine_sim_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in true_divide\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in true_divide\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in true_divide\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in true_divide\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in true_divide\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in true_divide\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in true_divide\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in true_divide\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "cosim_hT_nhF = get_something('hT_nhF', condition_seg_hist)\n",
    "cosim_hT_nhT = get_something('hT_nhT', condition_seg_hist)\n",
    "cosim_hF_nhF = get_something('hF_nhF', condition_seg_hist)\n",
    "cosim_hF_nhT = get_something('hF_nhT', condition_seg_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "F_onewayResult(statistic=0.0013339933011006825, pvalue=0.9999210503125144)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.f_oneway(cosim_hT_nhF, cosim_hT_nhT, cosim_hF_nhF, cosim_hF_nhT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
