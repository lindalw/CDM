{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lolab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim.downloader as api\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_google_300d = api.load('word2vec-google-news-300')\n",
    "wv_twitter_50d = api.load('glove-twitter-50')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cos similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9       ['-B-', 'okay', '.', 'i', \"'m\", 'done', 'anymo...\n",
       "23      ['-B-', 'i', 'have', 'the', 'same', 'picture',...\n",
       "60      ['-B-', 'guy', 'on', 'white', 'bike', '-A-', '...\n",
       "64      ['-B-', 'do', 'you', 'have', 'the', 'lady', 's...\n",
       "71      ['-B-', 'kk', 'little', 'girl', 'helping', 'da...\n",
       "                              ...                        \n",
       "2548    ['-B-', 'teenager', 'with', 'child', '-A-', 'b...\n",
       "2553     ['-B-', 'plaid', 'apron', '-A-', '<unk>', 'guy']\n",
       "2560    ['-A-', 'i', 'have', 'a', 'woman', 'with', 'a'...\n",
       "2571    ['-A-', 'black', 'shirt', 'red', 'lettering', ...\n",
       "2579    ['-B-', 'laptop', 'on', 'black', 'stand', 'and...\n",
       "Name: decoded_segment, Length: 263, dtype: object"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_df = pd.read_csv('history_predictions.csv')\n",
    "\n",
    "incorrect_predictions = history_df.loc[history_df['correct'] == 0]\n",
    "correct_predictions = history_df.loc[history_df['correct'] == 1]\n",
    "\n",
    "stopword_list = stopwords.words('english')\n",
    "\n",
    "stopword_list.append(\"-B-\")\n",
    "stopword_list.append(\"-A-\")\n",
    "stopword_list.append(\"<unk>\")\n",
    "\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "incorrect_predictions['decoded_segment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-8.92093033e-02, -2.45292753e-01,  8.20635110e-02, -3.35994102e-02,\n",
      "       -8.69635418e-02, -8.36643483e-03,  5.47086298e-01,  1.65184528e-01,\n",
      "        2.08654866e-01,  3.48535359e-01, -9.35202986e-02,  1.79868266e-01,\n",
      "       -3.28301764e+00,  2.66943052e-02, -1.21933129e-02,  1.85036898e-01,\n",
      "        9.95144770e-02, -3.34673285e-01, -1.51989043e-01, -1.23649687e-01,\n",
      "       -2.20028803e-01,  8.54320708e-04,  1.69558764e-01, -1.01894453e-01,\n",
      "        1.08210780e-02,  2.18606386e-02, -1.87982783e-01, -6.04968891e-02,\n",
      "        8.27350989e-02, -1.98729858e-01,  4.09521982e-02,  3.18558961e-01,\n",
      "        6.50063828e-02, -3.03752840e-01,  3.01999271e-01, -2.26621963e-02,\n",
      "       -9.62538645e-03,  2.74224356e-02,  6.34580925e-02, -1.15649833e-03,\n",
      "       -8.75078917e-01,  9.59849805e-02, -1.70730188e-01,  1.04005583e-01,\n",
      "        2.70360529e-01, -1.33330375e-02,  3.16587865e-01,  3.78532968e-02,\n",
      "       -8.57795961e-03,  2.58686971e-02], dtype=float32), array([-7.7965915e-02, -2.4658704e-01,  8.9228995e-02, -1.3395839e-03,\n",
      "       -5.7762135e-02,  7.4454513e-03,  5.6623149e-01,  1.4658287e-01,\n",
      "        1.6391486e-01,  3.5777223e-01, -8.8545762e-02,  1.5427582e-01,\n",
      "       -3.3331776e+00,  2.5745165e-02, -4.8072457e-02,  1.8893826e-01,\n",
      "        1.0002404e-01, -3.0951592e-01, -1.6836265e-01, -1.2899357e-01,\n",
      "       -1.9293144e-01, -3.3928271e-02,  1.8480772e-01, -7.3747657e-02,\n",
      "       -6.5139062e-03,  4.3665886e-02, -1.6343504e-01, -3.9170731e-02,\n",
      "        8.9509614e-02, -1.7884426e-01,  5.5378221e-02,  3.2410783e-01,\n",
      "        6.2983073e-02, -2.6157871e-01,  3.0168620e-01,  1.9647726e-03,\n",
      "       -3.9853193e-02,  2.2981739e-02,  5.4293681e-02, -1.8190850e-02,\n",
      "       -8.2512355e-01,  7.8838624e-02, -1.0217134e-01,  8.7860495e-02,\n",
      "        2.3825960e-01, -2.8230101e-02,  3.3539930e-01,  3.7539218e-02,\n",
      "       -2.7587852e-02,  3.5710376e-02], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "vectors = []\n",
    "\n",
    "# incorrect_predictions['decoded_segment'], decoded\n",
    "\n",
    "wrong_pred = incorrect_predictions['decoded_segment']\n",
    "good_pred = correct_predictions['decoded_segment']\n",
    "\n",
    "all_predictions = [good_pred, wrong_pred]\n",
    "\n",
    "\n",
    "for prediction in all_predictions:\n",
    "\n",
    "    vector = 0\n",
    "    i = 0\n",
    "    \n",
    "    for sentence in prediction:\n",
    "        for word in sentence.split(\",\"):\n",
    "\n",
    "            word = word.replace(\"'\", \"\")\n",
    "            word = word.replace(\" \", \"\")\n",
    "            word = word.lower()\n",
    "\n",
    "    #         if word is stopword, skip the word\n",
    "            if word in stopword_list:\n",
    "                continue\n",
    "\n",
    "            word = stemmer.stem(word)\n",
    "\n",
    "            try:\n",
    "\n",
    "                vector += wv_twitter_50d[word]\n",
    "                i += 1\n",
    "\n",
    "            except:\n",
    "                None\n",
    "                \n",
    "    vector = vector / i\n",
    "    vectors.append(vector)\n",
    "\n",
    "\n",
    "print(vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got scalar array instead:\narray=-0.05679996311664581.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-271-ed61146a2f93>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhiten\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\decomposition\\pca.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m         \"\"\"\n\u001b[1;32m--> 360\u001b[1;33m         \u001b[0mU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m         \u001b[0mU\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mU\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\decomposition\\pca.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m         X = check_array(X, dtype=[np.float64, np.float32], ensure_2d=True,\n\u001b[1;32m--> 382\u001b[1;33m                         copy=self.copy)\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[1;31m# Handle n_components==None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    512\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 514\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    515\u001b[0m             \u001b[1;31m# If input is 1D raise error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got scalar array instead:\narray=-0.05679996311664581.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "a = vectors[0].reshape(-1, 1)\n",
    "b = vectors[1].reshape(-1, 1)\n",
    "# print(a.shape)\n",
    "a = a.mean()\n",
    "b = b.mean()\n",
    "\n",
    "# print(type(a))\n",
    "\n",
    "# a = np.array([1, 4, 5, 765432, 5, 3])\n",
    "# a = a.reshape(-1, 1)\n",
    "\n",
    "pca = PCA(n_components=0.50, whiten = True)\n",
    "result = pca.fit_transform(a)\n",
    "result\n",
    "\n",
    "# result\n",
    "# pca = PCA(n_components=2)\n",
    "# # pca.fit(vectors)\n",
    "# # PCA(n_components=2)\n",
    "\n",
    "\n",
    "# result = pca.fit(vectors)\n",
    "\n",
    "# pca.explained_variance_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[1.e+00 2.e+00 2.e+04].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-251-993cbb4fdee4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\decomposition\\pca.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m         \"\"\"\n\u001b[1;32m--> 360\u001b[1;33m         \u001b[0mU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m         \u001b[0mU\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mU\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\decomposition\\pca.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m         X = check_array(X, dtype=[np.float64, np.float32], ensure_2d=True,\n\u001b[1;32m--> 382\u001b[1;33m                         copy=self.copy)\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[1;31m# Handle n_components==None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    519\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m         \u001b[1;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[1.e+00 2.e+00 2.e+04].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "a = [1, 2, 20000]\n",
    "b = [4, 2, 5434543]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit_transform(a,b )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tags distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lolab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\lolab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\lolab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\lolab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'grey': 'gray',\n",
       " 'ok.': 'OK',\n",
       " 't-shirt': 'tshirt',\n",
       " 'w/': 'with',\n",
       " 'frige': 'fridge',\n",
       " 'red/white': 'red and white',\n",
       " 'doughnuts': 'donuts',\n",
       " 'didnt': \"didn't\",\n",
       " 'ok..': 'ok',\n",
       " 'dr.': 'dr',\n",
       " 'no..': 'no',\n",
       " 'doughnut': 'donut',\n",
       " 'yes..': 'yes',\n",
       " 'yellow/orange': 'yellow and orange',\n",
       " 'pic..': 'picture',\n",
       " 'umberlla': 'umbrella',\n",
       " 'cest': 'chest',\n",
       " 'k..next': 'ok, next',\n",
       " 'surboard': 'surfboard',\n",
       " 'blue/white': 'blue and white',\n",
       " 'man/woman': 'man or woman',\n",
       " 'doesnt': \"doesn't\",\n",
       " 'scheppa': 'schepps',\n",
       " 'guy/girl': 'guy or girl',\n",
       " 'blue/green': 'blue and green',\n",
       " 'schepp': 'schepps',\n",
       " 'blue/black': 'blue and black',\n",
       " 'motarcycle': 'motorcycle',\n",
       " 'red/black': 'red and black',\n",
       " 'red/white/blue': 'red, white and blue',\n",
       " 'colour': 'color',\n",
       " 'close-up': 'closeup',\n",
       " 'green/yellow': 'green and yellow',\n",
       " 'red/pink': 'red and pink',\n",
       " 'buddist': 'buddhist',\n",
       " 'longsleeves': 'long sleeves',\n",
       " 'ihave': 'i have',\n",
       " 'yellow/green': 'yellow and green',\n",
       " 'no-': 'no',\n",
       " 'k.': 'ok',\n",
       " 'surfobard': 'surfboard',\n",
       " 'candleabra': 'candelabra',\n",
       " 'w/bear': 'with bear',\n",
       " 'olw': 'owl',\n",
       " 'white/pink': 'white and pink',\n",
       " 'pink/red': 'pink and red',\n",
       " 'squating': 'squatting',\n",
       " 'teddie': 'teddy',\n",
       " 'white/yellow': 'white and yellow',\n",
       " 'labtop': 'laptop',\n",
       " 'white/red': 'white and red',\n",
       " 'black/white': 'black and white',\n",
       " 'w/red': 'with red',\n",
       " 'yes/no': 'yes or no',\n",
       " 'all-blue': 'all blue',\n",
       " 'hilighted': 'highlighted',\n",
       " 'scoccer': 'soccer',\n",
       " 'shirt*': 'shirt',\n",
       " 'it*': 'it',\n",
       " 'cake..': 'cake',\n",
       " 'isnt': \"isn't\",\n",
       " 'pplaying': 'playing',\n",
       " 'pick-up': 'pickup',\n",
       " 'purple/blue': 'purpble and blue',\n",
       " 'bhudist': 'buddhist',\n",
       " 'van/truck': 'van or truck',\n",
       " 'common/different': 'common or different',\n",
       " 'plad': 'plaid',\n",
       " 'orange/yellow': 'orange and yellow',\n",
       " 'no..to': 'no to',\n",
       " 't-shirts': 'tshirts',\n",
       " 'orange/red': 'orange and red',\n",
       " 'yellow/white': 'yellow and white',\n",
       " 'hightlighted': 'highlighted',\n",
       " 'bike/green': 'green bike',\n",
       " 'train/bus': 'train or bus',\n",
       " 'orrange': 'orange',\n",
       " 'blue/purple': 'blue and purple',\n",
       " 'oh..': 'oh',\n",
       " 'holding/looking': 'holding and looking',\n",
       " 'truck/garbage': 'gargbage truck',\n",
       " 'mercedes-benz': 'mercedes_benz',\n",
       " 'coloured': 'colored',\n",
       " 'backgroun': 'background',\n",
       " 'one/': 'one',\n",
       " '.yes': 'yes',\n",
       " 'silver/black': 'silber and black',\n",
       " 'light-green': 'light green',\n",
       " 'bannanas': 'bananas',\n",
       " 'galss': 'glass',\n",
       " 'pokadot': 'polkadot',\n",
       " '*do': 'do',\n",
       " 'phoen': 'phone',\n",
       " 'palte': 'plaid',\n",
       " 'whuite': 'white',\n",
       " 'frnt': 'front',\n",
       " 'firdge': 'fridge',\n",
       " 'friut': 'fruit',\n",
       " 'silver/white': 'silber and white',\n",
       " 'bus/subway': 'bus and subway',\n",
       " 'long-sleeved': 'long sleeved',\n",
       " 'oven-': 'oven',\n",
       " 'frisbe': 'frisbee',\n",
       " 'nexxt': 'next',\n",
       " 'fedding': 'feeding',\n",
       " 'tan/white': 'tan or white',\n",
       " 'japenese': 'japanese',\n",
       " 'pinapple': 'pineapple',\n",
       " \"'road\": 'road',\n",
       " 'w/green': 'with green',\n",
       " 'whtie': 'white',\n",
       " 'hot-dog': 'hotdog',\n",
       " 'colorfull': 'colorful',\n",
       " 'highlited': 'highlighted',\n",
       " 'truck/bus': 'truck or bus',\n",
       " 'bycicle': 'bicycle',\n",
       " 'subway/train': 'subway or train',\n",
       " 'reask': 'ask again',\n",
       " 'baret': 'barret',\n",
       " 'u.s.': 'US',\n",
       " 'wiskers': 'whiskers',\n",
       " '..and': 'and',\n",
       " 'sirt': 'shirt',\n",
       " 'hat*': 'hat',\n",
       " 'motercycle': 'motorcycle',\n",
       " 'cak': 'cake',\n",
       " 'afgan': 'afghan',\n",
       " 'ok..next': 'ok, next',\n",
       " 'hant': \"hasn't\",\n",
       " 'haveguy': 'have a guy',\n",
       " 'shirt..': 'shirt',\n",
       " 'red/orange': 'red and orange',\n",
       " 'overn': 'oven',\n",
       " 'questions/': 'questions',\n",
       " 'cloth..': 'cloth',\n",
       " 'pot..': 'pot',\n",
       " 'handstanding': 'doing a handstand',\n",
       " 'nvm': 'never mind',\n",
       " 'sungalsses': 'sunglasses',\n",
       " 'bus/train': 'bus or train',\n",
       " 'now*': 'now',\n",
       " '*blue': 'blue',\n",
       " 'allyway': 'alleyway',\n",
       " 'unbrella': 'umbrella',\n",
       " 'filngers': 'fingers',\n",
       " 'bllue': 'blue',\n",
       " 'nmo': 'no',\n",
       " '*no': 'no',\n",
       " 'beer/muffin': 'beer and muffin',\n",
       " 'coool': 'cool',\n",
       " 'truck..': 'truck',\n",
       " 'right..': 'right',\n",
       " '*camera': 'camera',\n",
       " 'candels': 'candel',\n",
       " 'k..': 'ok',\n",
       " 'dad/daughter': 'dad and daughter',\n",
       " 'skirt*': 'skirt',\n",
       " 'cusions': 'cushions',\n",
       " 'furnityre': 'furniture',\n",
       " 'oky': 'okay',\n",
       " 'guy*': 'guy',\n",
       " 'banans': 'bananas',\n",
       " 'time/': 'time',\n",
       " 'manin': 'main',\n",
       " 'it..next': 'it. next',\n",
       " 'eatem': 'eaten',\n",
       " 'no..i': 'ni, i',\n",
       " 'k..thats': 'ok, thanks',\n",
       " 'suitecase': 'suitcase',\n",
       " 'helme': 'helmet',\n",
       " 'salvationarmy': 'salvation army',\n",
       " 'sandwitch': 'sandwich',\n",
       " 'standing/sitting': 'standing or sitting',\n",
       " 'bluie': 'blue',\n",
       " 'ansers': 'answers',\n",
       " 'baby/toddler': 'baby or toddler',\n",
       " 'orry': 'sorry',\n",
       " 'have*': 'have',\n",
       " 'frissbee': 'frisbee',\n",
       " 'child/toddler': 'child or toddler',\n",
       " 'orange/green': 'orange and green',\n",
       " 'gold/bronze': 'gold or bronze'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "import pickle\n",
    "from nltk import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "with open('oov_dictionary.pickle', 'rb') as f:\n",
    "    oov_dict = pickle.load(f)\n",
    "\n",
    "top_10 = ['NOUN', 'DET', 'VERB', 'ADP', 'ADJ', '.', 'ADV', 'NUM', 'PRON', 'CONJ']\n",
    "top_4 = ['NOUN', 'VERB', 'ADJ', 'ADV']\n",
    "\n",
    "''' explanation:\n",
    "\n",
    "ADJ\tadjective\tnew, good, high, special, big, local\n",
    "ADP\tadposition\ton, of, at, with, by, into, under\n",
    "ADV\tadverb\treally, already, still, early, now\n",
    "CONJ\tconjunction\tand, or, but, if, while, although\n",
    "DET\tdeterminer, article\tthe, a, some, most, every, no, which\n",
    "NOUN\tnoun\tyear, home, costs, time, Africa\n",
    "NUM\tnumeral\ttwenty-four, fourth, 1991, 14:24\n",
    "PRT\tparticle\tat, on, out, over per, that, up, with\n",
    "PRON\tpronoun\the, their, her, its, my, I, us\n",
    "VERB\tverb\tis, say, told, given, playing, would\n",
    ".\tpunctuation marks\t. , ; !\n",
    "X\tother\tersatz, esprit, dunno, gr8, univeristy\n",
    "\n",
    "'''\n",
    "\n",
    "oov_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pos_tag() got an unexpected keyword argument 'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-b4c0fe4d6cd6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mm_tokens_spell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moov_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mpos_tags\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm_tokens_spell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'universal'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# if len(pos_tags) == 0:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: pos_tag() got an unexpected keyword argument 'target'"
     ]
    }
   ],
   "source": [
    "# text = \" Guru99 is one of the best sites to learn WEB, SAP, Ethical Hacking and much more online.\"\n",
    "messages = ['ik ben bob en daar is het strand maar toen zei hij']\n",
    "pos_tags = []\n",
    "\n",
    "for message in messages:   \n",
    "    m_tokens = [i for i in word_tokenize(message.lower())]\n",
    "    \n",
    "    \n",
    "    m_tokens_spell = []\n",
    "    for word in m_tokens:\n",
    "        if word not in oov_dict:\n",
    "            m_tokens_spell.append(word)\n",
    "        else:\n",
    "            m_tokens_spell.append(oov_dict[word])\n",
    "            \n",
    "    pos_tags += [item[1] for item in pos_tag(m_tokens_spell, target='universal')]\n",
    "\n",
    "# if len(pos_tags) == 0:\n",
    "#     continue\n",
    "pos_tag_dict = dict()       \n",
    "for key,value in dict(Counter(pos_tags)).items():\n",
    "    pos_tag_dict[key] = [value]  \n",
    "    \n",
    "\n",
    "# pos_tag_dict['Round_Nr'] = round_nr\n",
    "# round_entry_df = pd.DataFrame.from_dict(pos_tag_dict)\n",
    "# pos_statistics.append(round_entry_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
