{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/emmahokken/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim.downloader as api\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
      "[==================================================] 100.0% 199.5/199.5MB downloaded\n"
     ]
    }
   ],
   "source": [
    "wv_google_300d = api.load('word2vec-google-news-300')\n",
    "wv_twitter_50d = api.load('glove-twitter-50')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising vocab from file.\n"
     ]
    }
   ],
   "source": [
    "from Vocab import Vocab\n",
    "vocab = Vocab('data/vocab.csv', 3)\n",
    "from get_predictions import get_predictions\n",
    "from helpers import *\n",
    "\n",
    "from ChainDataset import ChainDataset\n",
    "\n",
    "chain_test_set = ChainDataset(\n",
    "    data_dir='data/',\n",
    "    segment_file='segments.json',\n",
    "    chain_file='test_chains.json',\n",
    "    vectors_file='vectors.json',\n",
    "    split='test'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model: No history\n",
      "Initialising vocab from file.\n",
      "vocab len 3424\n",
      "Initialising vocab from file.\n",
      "Loaded seg2ranks and idlist\n",
      "params. normalize=True, mask=True, weight=5.5, weighting=True, batchsize=1, breaking=False\n",
      "Dataparams. data_dir=./data, segmentfile=segments.json, vectorfile=vectors.json, chains_file=chains.json\n",
      "processing test\n",
      "\n",
      "Start prediction\n",
      "predict no history\n",
      "{'segment': [5, 212, 4, 166, 5, 12, 22, 782, 29, 234, 213, 10, 4, 416, 14, 12, 22, 7, 6, 49, 11, 6, 155, 295, 11, 6, 811, 16, 9, 95, 10, 5, 17, 8, 12], 'image_set': ['93469', '380128', '14238', '259745', '524866', '341060', '483794', '96757'], 'targets': [5], 'length': 35, 'preds': tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]]), 'loss': tensor(4.9204), 'ranks': [0]}\n",
      "getting predicitons took 57.5736939907074 seconds\n",
      "loaded model: History\n",
      "Initialising vocab from file.\n",
      "vocab len 3424\n",
      "Initialising vocab from file.\n",
      "Loaded seg2ranks and idlist\n",
      "params. normalize=True, mask=True, weight=5.5, weighting=True, batchsize=1, breaking=False\n",
      "Dataparams. data_dir=./data, segmentfile=segments.json, vectorfile=vectors.json, chains_file=chains.json\n",
      "processing test\n",
      "\n",
      "Start prediction\n",
      "predicting datapoint 0 out of 6801\n",
      "predicting datapoint 500 out of 6801\n",
      "predicting datapoint 1000 out of 6801\n",
      "predicting datapoint 1500 out of 6801\n",
      "predicting datapoint 2000 out of 6801\n",
      "predicting datapoint 2500 out of 6801\n",
      "predicting datapoint 3000 out of 6801\n",
      "predicting datapoint 3500 out of 6801\n",
      "predicting datapoint 4000 out of 6801\n",
      "predicting datapoint 4500 out of 6801\n",
      "predicting datapoint 5000 out of 6801\n",
      "predicting datapoint 5500 out of 6801\n",
      "predicting datapoint 6000 out of 6801\n",
      "predicting datapoint 6500 out of 6801\n",
      "getting predicitons took 357.31360507011414 seconds\n"
     ]
    }
   ],
   "source": [
    "models_dict = {'History':'model_history_blind_accs_2019-02-20-14-22-23.pkl',\n",
    "            'No history': 'model_blind_accs_2019-02-17-21-18-7.pkl',\n",
    "                'No image': 'model_history_noimg_accs_2019-03-01-14-25-34.pkl'}\n",
    "# Also only uses the standard json files.\n",
    "dataset_pred_no_hist = get_predictions(model_name='No history', models_dict=models_dict)\n",
    "dataset_pred_hist = get_predictions(model_name='History', models_dict=models_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_pred_hist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1034b4aea18d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Create new history dataset with the segments in the same order as the no-history dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdataset_pred_hist_cp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreorder_datast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_pred_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minv_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Add the chain indices and the round in those chains per segement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset_pred_hist' is not defined"
     ]
    }
   ],
   "source": [
    "# Load in the segment ids that tell us which segment belongs to which index in the history dataset\n",
    "segment_ids = get_seg_ids()\n",
    "\n",
    "# Get inverted dict {seg_id:dataset_ind}\n",
    "inv_list = create_inv_list(segment_ids)\n",
    "\n",
    "# Create new history dataset with the segments in the same order as the no-history dataset\n",
    "dataset_pred_hist_cp = reorder_datast(dataset_pred_hist, inv_list)\n",
    "\n",
    "# Add the chain indices and the round in those chains per segement\n",
    "dataset_pred_no_hist, dataset_pred_hist_cp = add_chains_rounds(dataset_pred_no_hist, dataset_pred_hist_cp, chain_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = get_pred_dataframe(dataset_pred_no_hist, dataset_pred_hist_cp)\n",
    "conditions_inds = get_conditions_inds(dataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_condition_seg_hist(conditions_inds, dataset_pred_hist_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9       ['-B-', 'okay', '.', 'i', \"'m\", 'done', 'anymo...\n",
       "23      ['-B-', 'i', 'have', 'the', 'same', 'picture',...\n",
       "60      ['-B-', 'guy', 'on', 'white', 'bike', '-A-', '...\n",
       "64      ['-B-', 'do', 'you', 'have', 'the', 'lady', 's...\n",
       "71      ['-B-', 'kk', 'little', 'girl', 'helping', 'da...\n",
       "80      ['-B-', 'cool', 'common', 'ready', 'when', 'yo...\n",
       "110     ['-B-', 'i', 'have', 'a', 'person', 'pushing',...\n",
       "146     ['-B-', 'do', 'you', 'have', '2', 'people', 'r...\n",
       "159     ['-A-', 'i', 'have', 'that', 'also', '.', 'on'...\n",
       "176     ['-A-', 'my', 'second', 'one', 'is', 'a', 'hot...\n",
       "177     ['-A-', 'my', 'first', 'one', 'is', 'a', 'hot'...\n",
       "180     ['-A-', 'do', 'they', 'have', 'brown', 'hair',...\n",
       "185     ['-B-', 'and', 'do', 'you', 'have', 'stripe', ...\n",
       "187     ['-A-', 'i', 'have', 'a', 'woman', 'in', 'a', ...\n",
       "189     ['-A-', 'i', 'have', 'two', 'kids', 'carrying'...\n",
       "214     ['-B-', 'laptop', 'with', 'meal', 'and', 'drin...\n",
       "217     ['-B-', 'no', 'i', 'do', \"n't\", 'have', 'that'...\n",
       "219     ['-A-', 'what', 'about', 'the', 'guy', 'posing...\n",
       "222     ['-A-', 'hi', 'i', 'will', 'list', 'off', 'my'...\n",
       "228     ['-B-', 'do', 'you', 'have', 'a', 'woman', 'si...\n",
       "230     ['-A-', 'its', 'has', 'orange', 'and', 'grey',...\n",
       "235                          ['-A-', 'no', 'on', 'yours']\n",
       "240     ['-B-', 'salad', 'on', 'the', 'other', 'clear'...\n",
       "246     ['-B-', 'ok', 'cool', '.', '-A-', 'do', 'you',...\n",
       "248     ['-A-', 'the', 'cake', 'with', 'the', 'brown',...\n",
       "252     ['-B-', 'any', 'others', 'you', \"'d\", 'like', ...\n",
       "282     ['-B-', 'i', 'have', 'sleeping', 'baby', 'betw...\n",
       "308     ['-A-', 'man', 'in', 'black', 'shirt', 'on', '...\n",
       "317     ['-B-', 'hello', '!', 'do', 'you', 'have', 'a'...\n",
       "332     ['-B-', 'yes', '-A-', 'i', 'have', 'the', 'cak...\n",
       "                              ...                        \n",
       "2292    ['-A-', 'i', 'have', 'a', '[', 'picture', 'wit...\n",
       "2303    ['-A-', 'got', 'a', 'dog', 'laying', 'its', 'h...\n",
       "2306    ['-B-', 'dog', 'upside', 'down', 'in', 'person...\n",
       "2312    ['-B-', 'excellent', '.', '``', 'common', \"''\"...\n",
       "2313    ['-A-', 'the', 'table', 'with', 'the', '``', '...\n",
       "2318    ['-A-', 'all', 'of', 'my', 'photos', 'are', '<...\n",
       "2327    ['-B-', 'proceed', '-A-', 'tow', 'ladies', 'ea...\n",
       "2328    ['-A-', 'i', 'have', 'a', 'woman', 'and', 'a',...\n",
       "2341    ['-B-', 'a', 'woman', 'in', 'a', 'hat', 'and',...\n",
       "2345    ['-A-', 'i', 'have', 'one', 'bus', 'being', 't...\n",
       "2368                  ['-B-', 'not', 'sure', '-A-', 'no']\n",
       "2370    ['-B-', '5', 'people', 'in', 'green', 'vests',...\n",
       "2372    ['-B-', 'i', 'submitted', '-A-', 'helmet', 'ma...\n",
       "2384    ['-B-', 'hamburger', 'with', 'black', 'mug', '...\n",
       "2425    ['-B-', 'pink', 'bowl', 'rice', 'and', 'brocco...\n",
       "2447                         ['-B-', 'i', 'have', 'that']\n",
       "2456                                ['-B-', '<unk>', '.']\n",
       "2466                        ['-A-', 'now', '-B-', 'nope']\n",
       "2476    ['-B-', 'red', 'truck', 'with', 'possibly', 'a...\n",
       "2479    ['-A-', 'hello', '-B-', 'hello', '.', 'do', 'y...\n",
       "2492    ['-A-', 'yes', 'to', 'dog', 'with', 'wine', 'g...\n",
       "2499    ['-B-', 'he', \"'s\", 'holding', 'a', 'plate', '...\n",
       "2501    ['-B-', 'any', 'more', 'questions', '?', '-A-'...\n",
       "2528    ['-A-', 'do', 'you', 'have', 'the', 'same', 'g...\n",
       "2531    ['-B-', 'i', 'have', 'kitchen', 'with', 'candl...\n",
       "2548    ['-B-', 'teenager', 'with', 'child', '-A-', 'b...\n",
       "2553     ['-B-', 'plaid', 'apron', '-A-', '<unk>', 'guy']\n",
       "2560    ['-A-', 'i', 'have', 'a', 'woman', 'with', 'a'...\n",
       "2571    ['-A-', 'black', 'shirt', 'red', 'lettering', ...\n",
       "2579    ['-B-', 'laptop', 'on', 'black', 'stand', 'and...\n",
       "Name: decoded_segment, Length: 263, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_df = pd.read_csv('history_predictions.csv')\n",
    "\n",
    "incorrect_predictions = history_df.loc[history_df['correct'] == 0]\n",
    "correct_predictions = history_df.loc[history_df['correct'] == 1]\n",
    "\n",
    "stopword_list = stopwords.words('english')\n",
    "\n",
    "stopword_list.append(\"-B-\")\n",
    "stopword_list.append(\"-A-\")\n",
    "stopword_list.append(\"<unk>\")\n",
    "\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "incorrect_predictions['decoded_segment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-8.92508179e-02, -2.45265096e-01,  8.19780305e-02, -3.33554409e-02,\n",
      "       -8.68604779e-02, -8.33939482e-03,  5.46882093e-01,  1.65179178e-01,\n",
      "        2.08532125e-01,  3.48601133e-01, -9.33928862e-02,  1.79920807e-01,\n",
      "       -3.28285956e+00,  2.69561689e-02, -1.22719491e-02,  1.84684321e-01,\n",
      "        9.91539881e-02, -3.34496498e-01, -1.51952639e-01, -1.23826720e-01,\n",
      "       -2.20019653e-01,  5.86802664e-04,  1.69557005e-01, -1.01809360e-01,\n",
      "        1.09491479e-02,  2.17816290e-02, -1.87915027e-01, -6.05420507e-02,\n",
      "        8.28245729e-02, -1.98522657e-01,  4.09429595e-02,  3.18477005e-01,\n",
      "        6.46469221e-02, -3.03640574e-01,  3.02216321e-01, -2.25010961e-02,\n",
      "       -9.80426744e-03,  2.73617413e-02,  6.34614006e-02, -1.49778475e-03,\n",
      "       -8.75186801e-01,  9.57668126e-02, -1.70625001e-01,  1.03947945e-01,\n",
      "        2.70428836e-01, -1.31704006e-02,  3.16535622e-01,  3.77877802e-02,\n",
      "       -8.66262801e-03,  2.58087534e-02], dtype=float32), array([-7.83506781e-02, -2.46343359e-01,  8.84689391e-02,  7.23917794e-04,\n",
      "       -5.69216907e-02,  7.64957350e-03,  5.64410567e-01,  1.46573693e-01,\n",
      "        1.62934139e-01,  3.58327717e-01, -8.74441713e-02,  1.54785633e-01,\n",
      "       -3.33169794e+00,  2.80313119e-02, -4.86864373e-02,  1.85854897e-01,\n",
      "        9.68782753e-02, -3.08024526e-01, -1.68012261e-01, -1.30527034e-01,\n",
      "       -1.92906201e-01, -3.61921489e-02,  1.84761792e-01, -7.30618611e-02,\n",
      "       -5.36195328e-03,  4.29329015e-02, -1.62893161e-01, -3.96073982e-02,\n",
      "        9.02767777e-02, -1.77076757e-01,  5.52686751e-02,  3.23381811e-01,\n",
      "        5.98512292e-02, -2.60684252e-01,  3.03580254e-01,  3.32069048e-03,\n",
      "       -4.13529463e-02,  2.24611815e-02,  5.43408729e-02, -2.11338662e-02,\n",
      "       -8.26164722e-01,  7.69699141e-02, -1.01391435e-01,  8.73902664e-02,\n",
      "        2.38919124e-01, -2.67813802e-02,  3.34905833e-01,  3.69682647e-02,\n",
      "       -2.82883067e-02,  3.51676829e-02], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "vectors = []\n",
    "\n",
    "# incorrect_predictions['decoded_segment'], decoded\n",
    "\n",
    "wrong_pred = incorrect_predictions['decoded_segment']\n",
    "good_pred = correct_predictions['decoded_segment']\n",
    "\n",
    "all_predictions = [good_pred, wrong_pred]\n",
    "\n",
    "\n",
    "for prediction in all_predictions:\n",
    "\n",
    "    vector = 0\n",
    "    i = 0\n",
    "    \n",
    "    for sentence in prediction:\n",
    "        for word in sentence.split(\",\"):\n",
    "\n",
    "            word = word.replace(\"'\", \"\")\n",
    "            word = word.replace(\" \", \"\")\n",
    "            word = word.lower()\n",
    "\n",
    "    #         if word is stopword, skip the word\n",
    "            if word in stopword_list:\n",
    "                continue\n",
    "\n",
    "            word = stemmer.stem(word)\n",
    "\n",
    "            try:\n",
    "\n",
    "                vector += wv_twitter_50d[word]\n",
    "                i += 1\n",
    "\n",
    "            except:\n",
    "                None\n",
    "                \n",
    "    vector = vector / i\n",
    "    vectors.append(vector)\n",
    "\n",
    "\n",
    "print(vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got scalar array instead:\narray=-0.056815993040800095.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ed61146a2f93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhiten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/pca.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \"\"\"\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m         \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/pca.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         X = check_array(X, dtype=[np.float64, np.float32], ensure_2d=True,\n\u001b[0;32m--> 381\u001b[0;31m                         copy=self.copy)\n\u001b[0m\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;31m# Handle n_components==None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    543\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    546\u001b[0m             \u001b[0;31m# If input is 1D raise error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got scalar array instead:\narray=-0.056815993040800095.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "a = vectors[0].reshape(-1, 1)\n",
    "b = vectors[1].reshape(-1, 1)\n",
    "# print(a.shape)\n",
    "a = a.mean()\n",
    "b = b.mean()\n",
    "\n",
    "# print(type(a))\n",
    "\n",
    "# a = np.array([1, 4, 5, 765432, 5, 3])\n",
    "# a = a.reshape(-1, 1)\n",
    "\n",
    "pca = PCA(n_components=0.50, whiten = True)\n",
    "result = pca.fit_transform(a)\n",
    "result\n",
    "\n",
    "# result\n",
    "# pca = PCA(n_components=2)\n",
    "# # pca.fit(vectors)\n",
    "# # PCA(n_components=2)\n",
    "\n",
    "\n",
    "# result = pca.fit(vectors)\n",
    "\n",
    "# pca.explained_variance_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[1.e+00 2.e+00 2.e+04].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-251-993cbb4fdee4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\decomposition\\pca.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m         \"\"\"\n\u001b[1;32m--> 360\u001b[1;33m         \u001b[0mU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m         \u001b[0mU\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mU\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\decomposition\\pca.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m         X = check_array(X, dtype=[np.float64, np.float32], ensure_2d=True,\n\u001b[1;32m--> 382\u001b[1;33m                         copy=self.copy)\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[1;31m# Handle n_components==None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    519\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m         \u001b[1;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[1.e+00 2.e+00 2.e+04].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "a = [1, 2, 20000]\n",
    "b = [4, 2, 5434543]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit_transform(a,b )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tags distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lolab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\lolab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\lolab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\lolab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'grey': 'gray',\n",
       " 'ok.': 'OK',\n",
       " 't-shirt': 'tshirt',\n",
       " 'w/': 'with',\n",
       " 'frige': 'fridge',\n",
       " 'red/white': 'red and white',\n",
       " 'doughnuts': 'donuts',\n",
       " 'didnt': \"didn't\",\n",
       " 'ok..': 'ok',\n",
       " 'dr.': 'dr',\n",
       " 'no..': 'no',\n",
       " 'doughnut': 'donut',\n",
       " 'yes..': 'yes',\n",
       " 'yellow/orange': 'yellow and orange',\n",
       " 'pic..': 'picture',\n",
       " 'umberlla': 'umbrella',\n",
       " 'cest': 'chest',\n",
       " 'k..next': 'ok, next',\n",
       " 'surboard': 'surfboard',\n",
       " 'blue/white': 'blue and white',\n",
       " 'man/woman': 'man or woman',\n",
       " 'doesnt': \"doesn't\",\n",
       " 'scheppa': 'schepps',\n",
       " 'guy/girl': 'guy or girl',\n",
       " 'blue/green': 'blue and green',\n",
       " 'schepp': 'schepps',\n",
       " 'blue/black': 'blue and black',\n",
       " 'motarcycle': 'motorcycle',\n",
       " 'red/black': 'red and black',\n",
       " 'red/white/blue': 'red, white and blue',\n",
       " 'colour': 'color',\n",
       " 'close-up': 'closeup',\n",
       " 'green/yellow': 'green and yellow',\n",
       " 'red/pink': 'red and pink',\n",
       " 'buddist': 'buddhist',\n",
       " 'longsleeves': 'long sleeves',\n",
       " 'ihave': 'i have',\n",
       " 'yellow/green': 'yellow and green',\n",
       " 'no-': 'no',\n",
       " 'k.': 'ok',\n",
       " 'surfobard': 'surfboard',\n",
       " 'candleabra': 'candelabra',\n",
       " 'w/bear': 'with bear',\n",
       " 'olw': 'owl',\n",
       " 'white/pink': 'white and pink',\n",
       " 'pink/red': 'pink and red',\n",
       " 'squating': 'squatting',\n",
       " 'teddie': 'teddy',\n",
       " 'white/yellow': 'white and yellow',\n",
       " 'labtop': 'laptop',\n",
       " 'white/red': 'white and red',\n",
       " 'black/white': 'black and white',\n",
       " 'w/red': 'with red',\n",
       " 'yes/no': 'yes or no',\n",
       " 'all-blue': 'all blue',\n",
       " 'hilighted': 'highlighted',\n",
       " 'scoccer': 'soccer',\n",
       " 'shirt*': 'shirt',\n",
       " 'it*': 'it',\n",
       " 'cake..': 'cake',\n",
       " 'isnt': \"isn't\",\n",
       " 'pplaying': 'playing',\n",
       " 'pick-up': 'pickup',\n",
       " 'purple/blue': 'purpble and blue',\n",
       " 'bhudist': 'buddhist',\n",
       " 'van/truck': 'van or truck',\n",
       " 'common/different': 'common or different',\n",
       " 'plad': 'plaid',\n",
       " 'orange/yellow': 'orange and yellow',\n",
       " 'no..to': 'no to',\n",
       " 't-shirts': 'tshirts',\n",
       " 'orange/red': 'orange and red',\n",
       " 'yellow/white': 'yellow and white',\n",
       " 'hightlighted': 'highlighted',\n",
       " 'bike/green': 'green bike',\n",
       " 'train/bus': 'train or bus',\n",
       " 'orrange': 'orange',\n",
       " 'blue/purple': 'blue and purple',\n",
       " 'oh..': 'oh',\n",
       " 'holding/looking': 'holding and looking',\n",
       " 'truck/garbage': 'gargbage truck',\n",
       " 'mercedes-benz': 'mercedes_benz',\n",
       " 'coloured': 'colored',\n",
       " 'backgroun': 'background',\n",
       " 'one/': 'one',\n",
       " '.yes': 'yes',\n",
       " 'silver/black': 'silber and black',\n",
       " 'light-green': 'light green',\n",
       " 'bannanas': 'bananas',\n",
       " 'galss': 'glass',\n",
       " 'pokadot': 'polkadot',\n",
       " '*do': 'do',\n",
       " 'phoen': 'phone',\n",
       " 'palte': 'plaid',\n",
       " 'whuite': 'white',\n",
       " 'frnt': 'front',\n",
       " 'firdge': 'fridge',\n",
       " 'friut': 'fruit',\n",
       " 'silver/white': 'silber and white',\n",
       " 'bus/subway': 'bus and subway',\n",
       " 'long-sleeved': 'long sleeved',\n",
       " 'oven-': 'oven',\n",
       " 'frisbe': 'frisbee',\n",
       " 'nexxt': 'next',\n",
       " 'fedding': 'feeding',\n",
       " 'tan/white': 'tan or white',\n",
       " 'japenese': 'japanese',\n",
       " 'pinapple': 'pineapple',\n",
       " \"'road\": 'road',\n",
       " 'w/green': 'with green',\n",
       " 'whtie': 'white',\n",
       " 'hot-dog': 'hotdog',\n",
       " 'colorfull': 'colorful',\n",
       " 'highlited': 'highlighted',\n",
       " 'truck/bus': 'truck or bus',\n",
       " 'bycicle': 'bicycle',\n",
       " 'subway/train': 'subway or train',\n",
       " 'reask': 'ask again',\n",
       " 'baret': 'barret',\n",
       " 'u.s.': 'US',\n",
       " 'wiskers': 'whiskers',\n",
       " '..and': 'and',\n",
       " 'sirt': 'shirt',\n",
       " 'hat*': 'hat',\n",
       " 'motercycle': 'motorcycle',\n",
       " 'cak': 'cake',\n",
       " 'afgan': 'afghan',\n",
       " 'ok..next': 'ok, next',\n",
       " 'hant': \"hasn't\",\n",
       " 'haveguy': 'have a guy',\n",
       " 'shirt..': 'shirt',\n",
       " 'red/orange': 'red and orange',\n",
       " 'overn': 'oven',\n",
       " 'questions/': 'questions',\n",
       " 'cloth..': 'cloth',\n",
       " 'pot..': 'pot',\n",
       " 'handstanding': 'doing a handstand',\n",
       " 'nvm': 'never mind',\n",
       " 'sungalsses': 'sunglasses',\n",
       " 'bus/train': 'bus or train',\n",
       " 'now*': 'now',\n",
       " '*blue': 'blue',\n",
       " 'allyway': 'alleyway',\n",
       " 'unbrella': 'umbrella',\n",
       " 'filngers': 'fingers',\n",
       " 'bllue': 'blue',\n",
       " 'nmo': 'no',\n",
       " '*no': 'no',\n",
       " 'beer/muffin': 'beer and muffin',\n",
       " 'coool': 'cool',\n",
       " 'truck..': 'truck',\n",
       " 'right..': 'right',\n",
       " '*camera': 'camera',\n",
       " 'candels': 'candel',\n",
       " 'k..': 'ok',\n",
       " 'dad/daughter': 'dad and daughter',\n",
       " 'skirt*': 'skirt',\n",
       " 'cusions': 'cushions',\n",
       " 'furnityre': 'furniture',\n",
       " 'oky': 'okay',\n",
       " 'guy*': 'guy',\n",
       " 'banans': 'bananas',\n",
       " 'time/': 'time',\n",
       " 'manin': 'main',\n",
       " 'it..next': 'it. next',\n",
       " 'eatem': 'eaten',\n",
       " 'no..i': 'ni, i',\n",
       " 'k..thats': 'ok, thanks',\n",
       " 'suitecase': 'suitcase',\n",
       " 'helme': 'helmet',\n",
       " 'salvationarmy': 'salvation army',\n",
       " 'sandwitch': 'sandwich',\n",
       " 'standing/sitting': 'standing or sitting',\n",
       " 'bluie': 'blue',\n",
       " 'ansers': 'answers',\n",
       " 'baby/toddler': 'baby or toddler',\n",
       " 'orry': 'sorry',\n",
       " 'have*': 'have',\n",
       " 'frissbee': 'frisbee',\n",
       " 'child/toddler': 'child or toddler',\n",
       " 'orange/green': 'orange and green',\n",
       " 'gold/bronze': 'gold or bronze'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "import pickle\n",
    "from nltk import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "with open('oov_dictionary.pickle', 'rb') as f:\n",
    "    oov_dict = pickle.load(f)\n",
    "\n",
    "top_10 = ['NOUN', 'DET', 'VERB', 'ADP', 'ADJ', '.', 'ADV', 'NUM', 'PRON', 'CONJ']\n",
    "top_4 = ['NOUN', 'VERB', 'ADJ', 'ADV']\n",
    "\n",
    "''' explanation:\n",
    "\n",
    "ADJ\tadjective\tnew, good, high, special, big, local\n",
    "ADP\tadposition\ton, of, at, with, by, into, under\n",
    "ADV\tadverb\treally, already, still, early, now\n",
    "CONJ\tconjunction\tand, or, but, if, while, although\n",
    "DET\tdeterminer, article\tthe, a, some, most, every, no, which\n",
    "NOUN\tnoun\tyear, home, costs, time, Africa\n",
    "NUM\tnumeral\ttwenty-four, fourth, 1991, 14:24\n",
    "PRT\tparticle\tat, on, out, over per, that, up, with\n",
    "PRON\tpronoun\the, their, her, its, my, I, us\n",
    "VERB\tverb\tis, say, told, given, playing, would\n",
    ".\tpunctuation marks\t. , ; !\n",
    "X\tother\tersatz, esprit, dunno, gr8, univeristy\n",
    "\n",
    "'''\n",
    "\n",
    "oov_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pos_tag() got an unexpected keyword argument 'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-b4c0fe4d6cd6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mm_tokens_spell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moov_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mpos_tags\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm_tokens_spell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'universal'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# if len(pos_tags) == 0:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: pos_tag() got an unexpected keyword argument 'target'"
     ]
    }
   ],
   "source": [
    "# text = \" Guru99 is one of the best sites to learn WEB, SAP, Ethical Hacking and much more online.\"\n",
    "messages = ['ik ben bob en daar is het strand maar toen zei hij']\n",
    "pos_tags = []\n",
    "\n",
    "for message in messages:   \n",
    "    m_tokens = [i for i in word_tokenize(message.lower())]\n",
    "    \n",
    "    \n",
    "    m_tokens_spell = []\n",
    "    for word in m_tokens:\n",
    "        if word not in oov_dict:\n",
    "            m_tokens_spell.append(word)\n",
    "        else:\n",
    "            m_tokens_spell.append(oov_dict[word])\n",
    "            \n",
    "    pos_tags += [item[1] for item in pos_tag(m_tokens_spell, target='universal')]\n",
    "\n",
    "# if len(pos_tags) == 0:\n",
    "#     continue\n",
    "pos_tag_dict = dict()       \n",
    "for key,value in dict(Counter(pos_tags)).items():\n",
    "    pos_tag_dict[key] = [value]  \n",
    "    \n",
    "\n",
    "# pos_tag_dict['Round_Nr'] = round_nr\n",
    "# round_entry_df = pd.DataFrame.from_dict(pos_tag_dict)\n",
    "# pos_statistics.append(round_entry_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
